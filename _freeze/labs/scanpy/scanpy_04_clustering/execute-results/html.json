{
  "hash": "947f13e5bac106e90875196d8f4f9fce",
  "result": {
    "markdown": "---\ntitle: \"{{< meta clust_title >}}\"\nsubtitle: \"{{< meta subtitle_scanpy >}}\"\ndescription: \"{{< meta clust_description >}}\"\nformat: html\nengine: jupyter\n---\n\n::: {.callout-note}\nCode chunks run Python commands unless it starts with `%%bash`, in which case, those chunks run shell commands.\n:::\n\nIn this tutorial we will continue the analysis of the integrated dataset. We will use the scanpy enbedding to perform the clustering using graph community detection algorithms.\n\n\n{{< meta clust_2 >}}\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=80)\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama.h5ad\"\nif not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr_scanorama.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nAnnData object with n_obs × n_vars = 5725 × 2727\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'sample_colors', 'tsne', 'umap'\n    obsm: 'Scanorama', 'X_pca', 'X_tsne', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n```\n:::\n:::\n\n\n## {{< meta clust_graphclust >}}\n\n\n{{< meta clust_graphclust_1 >}}\n\n\n\nIf you recall from the integration, we already constructed a knn graph before running UMAP. Hence we do not need to do it again, and can run the community detection right away.\n\nThe modularity optimization algoritm in Scanpy are *Leiden* and *Louvain*. Lets test both and see how they compare.\n\n### Leiden\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nsc.tl.leiden(adata, key_added = \"leiden_1.0\") # default resolution in 1.0\nsc.tl.leiden(adata, resolution = 0.6, key_added = \"leiden_0.6\")\nsc.tl.leiden(adata, resolution = 0.4, key_added = \"leiden_0.4\")\nsc.tl.leiden(adata, resolution = 1.4, key_added = \"leiden_1.4\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrunning Leiden clustering\n    finished: found 16 clusters and added\n    'leiden_1.0', the cluster labels (adata.obs, categorical) (0:00:01)\nrunning Leiden clustering\n    finished: found 12 clusters and added\n    'leiden_0.6', the cluster labels (adata.obs, categorical) (0:00:01)\nrunning Leiden clustering\n    finished: found 10 clusters and added\n    'leiden_0.4', the cluster labels (adata.obs, categorical) (0:00:01)\nrunning Leiden clustering\n    finished: found 18 clusters and added\n    'leiden_1.4', the cluster labels (adata.obs, categorical) (0:00:02)\n```\n:::\n:::\n\n\nPlot the clusters, as you can see, with increased resolution, we get higher granularity in the clustering.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nsc.pl.umap(adata, color=['leiden_0.4', 'leiden_0.6', 'leiden_1.0','leiden_1.4'])\n```\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nOnce we have done clustering, the relationships between clusters can be calculated as correlation in PCA space and we also visualize some of the marker genes that we used in the Dim Reduction lab onto the clusters.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsc.tl.dendrogram(adata, groupby = \"leiden_0.6\")\nsc.pl.dendrogram(adata, groupby = \"leiden_0.6\")\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"FCGR3A\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR1A\"]\nsc.pl.dotplot(adata, genes, groupby='leiden_0.6', dendrogram=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_leiden_0.6']`\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-6-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-6-output-3.png){}\n:::\n:::\n\n\nPlot proportion of cells from each condition per cluster.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['type'], normalize='index')\ntmp.plot.bar(stacked=True).legend(loc='upper right')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.legend.Legend at 0x7fff544c5cf0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n### Louvain\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsc.tl.louvain(adata, key_added = \"louvain_1.0\") # default resolution in 1.0\nsc.tl.louvain(adata, resolution = 0.6, key_added = \"louvain_0.6\")\nsc.tl.louvain(adata, resolution = 0.4, key_added = \"louvain_0.4\")\nsc.tl.louvain(adata, resolution = 1.4, key_added = \"louvain_1.4\")\n\nsc.pl.umap(adata, color=['louvain_0.4', 'louvain_0.6', 'louvain_1.0','louvain_1.4'])\n\nsc.tl.dendrogram(adata, groupby = \"louvain_0.6\")\nsc.pl.dendrogram(adata, groupby = \"louvain_0.6\")\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"FCGR3A\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR1A\"]\n\nsc.pl.dotplot(adata, genes, groupby='louvain_0.6', dendrogram=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 12 clusters and added\n    'louvain_1.0', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 9 clusters and added\n    'louvain_0.6', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 7 clusters and added\n    'louvain_0.4', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 17 clusters and added\n    'louvain_1.4', the cluster labels (adata.obs, categorical) (0:00:00)\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_louvain_0.6']`\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-8-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-8-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-8-output-4.png){}\n:::\n:::\n\n\n## {{< meta clust_kmean >}}\n\n\n{{< meta clust_kmean_1 >}}\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# extract pca coordinates\nX_pca = adata.obsm['Scanorama'] \n\n# kmeans with k=5\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X_pca) \nadata.obs['kmeans5'] = kmeans.labels_.astype(str)\n\n# kmeans with k=10\nkmeans = KMeans(n_clusters=10, random_state=0).fit(X_pca) \nadata.obs['kmeans10'] = kmeans.labels_.astype(str)\n\n# kmeans with k=15\nkmeans = KMeans(n_clusters=15, random_state=0).fit(X_pca)\nadata.obs['kmeans15'] = kmeans.labels_.astype(str)\n\nsc.pl.umap(adata, color=['kmeans5', 'kmeans10', 'kmeans15'])\n\nadata.obsm\n```\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-9-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nAxisArrays with keys: Scanorama, X_pca, X_tsne, X_umap\n```\n:::\n:::\n\n\n## {{< meta clust_hier >}}\n\nHierarchical clustering is another generic form of clustering that can be applied also to scRNA-seq data. As K-means, it is typically applied to a reduced dimension representation of the data. Hierarchical clustering returns an entire hierarchy of partitionings (a dendrogram) that can be cut at different levels. Hierarchical clustering is done in these steps:\n\n1. Define the distances between samples. The most common are Euclidean distance (a.k.a. straight line between two points) or correlation coefficients.\n2. Define a measure of distances between clusters, called *linkage* criteria. It can for example be average distances between clusters. Commonly used methods are `single`, `complete`, `average`, `median`, `centroid` and `ward`.\n3. Define the dendrogram among all samples using **Bottom-up** or **Top-down** approach. **Bottom-up** is where samples start with their own cluster which end up merged pair-by-pair until only one cluster is left. **Top-down** is where samples start all in the same cluster that end up being split by 2 until each sample has its own cluster.\n\n\n{{< meta clust_hier_dist_2 >}}\n\n\n\nThe function `AgglomerativeClustering` has the option of running with disntance metrics “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed\". However, with ward linkage only euklidean distances works. Here we will try out euclidean distance and ward linkage calculated in PCA space.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\nadata.obs['hclust_5'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=10, affinity='euclidean', linkage='ward')\nadata.obs['hclust_10'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=15, affinity='euclidean', linkage='ward')\nadata.obs['hclust_15'] = cluster.fit_predict(X_pca).astype(str)\n\nsc.pl.umap(adata, color=['hclust_5', 'hclust_10', 'hclust_15'])\n```\n\n::: {.cell-output .cell-output-display}\n![](scanpy_04_clustering_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n{{< meta clust_save >}}\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nadata.write_h5ad('./data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad')\n```\n:::\n\n\n:::{.callout-note title=\"Discuss\"}\n\n{{< meta clust_3 >}}\n\n\n:::\n\n## {{< meta session >}}\n\n<details>\n  <summary>Click here</summary>\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsc.logging.print_versions()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-----\nanndata     0.10.3\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.2\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.1.4\nparso               0.8.3\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.11.4\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.3.2\nsniffio             1.3.0\nsocks               1.7.1\nstack_data          0.6.2\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-6.5.11-linuxkit-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-01-07 15:50\n```\n:::\n:::\n\n\n</details>\n\n",
    "supporting": [
      "scanpy_04_clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}