---
output: html_document
editor_options: 
  chunk_output_type: console
---

#CSS_ALL:author: "Åsa Björklund  &  Paulo Czarnewski"
#CSS_ALL:date: "Sept 13, 2019"
#CSS_ALL:output:
#CSS_ALL:  html_document:
#CSS_ALL:    self_contained: true
#CSS_ALL:    highlight: tango
#CSS_ALL:    df_print: paged
#CSS_ALL:    toc: yes
#CSS_ALL:    toc_float:
#CSS_ALL:      collapsed: false
#CSS_ALL:      smooth_scroll: true
#CSS_ALL:    toc_depth: 3
#CSS_ALL:    keep_md: yes
#CSS_ALL:    fig_caption: true
#CSS_ALL:  html_notebook:
#CSS_ALL:    self_contained: true
#CSS_ALL:    highlight: tango
#CSS_ALL:    df_print: paged
#CSS_ALL:    toc: yes
#CSS_ALL:    toc_float:
#CSS_ALL:      collapsed: false
#CSS_ALL:      smooth_scroll: true
#CSS_ALL:    toc_depth: 3


#CHUNK_OPT:```{r setup, include=FALSE}
#CHUNK_OPT:knitr::opts_chunk$set(message=FALSE, warning=FALSE, result='hold',fig.width=12,tidy=TRUE)
#CHUNK_OPT:```
#CHUNK_OPT:<style>
#CHUNK_OPT:h1, .h1, h2, .h2, h3, .h3, h4, .h4 { margin-top: 50px }
#CHUNK_OPT:p.caption {font-size: 0.9em;font-style: italic;color: grey;margin-right: 10%;margin-left: 10%;text-align: justify}
#CHUNK_OPT:</style>


#define path to this script (wherever it is)
```{r setup, include=FALSE}
initial.options <- commandArgs(trailingOnly = FALSE)
script_path <- dirname(sub("--file=","",initial.options[grep("--file=",initial.options)]))
setwd(script_path)

dir.create("compiled/seurat",recursive = T, showWarnings = F)
dir.create("compiled/scater",recursive = T, showWarnings = F)
dir.create("compiled/scanpy",recursive = T, showWarnings = F)
```






#------------------#
#   SESSION INFO   #
#------------------#
#SESSION_INFO:### Session Info
#SESSION_INFO:***
#-----------



#----------------------------#
#   GET LIBRARIES AND DATA   #
#----------------------------#
#DATA_TITLE:# Get data

#DATA_ALL1:In this tutorial, we will be using 3 publicly available dataset downloaded from 10X Genomics repository. They can be downloaded using the following bash commands. Simply create a folder called `data` and then use `curl` to pull the data from the 10X database.

#DATA_ALL2:With data in place, now we can start loading libraries we will use in this tutorial.

#DATA_ALL3:We can first load the data individually by reading directly from HDF5 file format (.h5). Note that among those , the dataset p3.1k actually has both gene expression and CITE-seq data, so we will use only the `Gene Expression` here.

#DATA_SEURAT:
#DATA_SCRAN:
#DATA_SCRANPY:
#-----------




#-------------------#
#   CREATE OBJECT   #
#-------------------#
#OBJ_TITLE:# Create one merged object
#OBJ_ALL1:We can now load the expression matricies into objects and then merge them into a single merged object. Each analysis workflow (Seurat, Scater, Scranpy, etc) has its own way of storing data. We will add dataset labels as cell.ids just in case you have overlapping barcodes between the datasets. After that we add a column `Chemistry` in the metadata for plotting later on.

#OBJ_ALL2: Here it is how the count matrix and the metatada look like for every cell.

#OBJ_SEURAT:
#OBJ_SCRAN:
#OBJ_SCRANPY: You can print a summary of the datasets in the Scanpy object, or a summary of the whole object.
#-----------



#--------#
#   QC   #
#--------#
#QC_TITLE:# Calculate QC

#QC_ALL1:Having the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitocondrial and ribosomal genes per cell and add to the metadata. This will be helpfull to visualize them across different metadata parameteres (i.e. datasetID and chemistry version). There are several ways of doing this, and here manually calculate the proportion of mitochondrial reads and add to the metadata table.

#QC_ALL1.1:Citing from "Simple Single Cell" workflows (Lun, McCarthy & Marioni, 2017): "High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane."

#QC_1_SCANPY: First, let Scanpy calculate some general qc-stats for genes and cells with the function `sc.pp.calculate_qc_metrics`, similar to `calculateQCmetrics` in Scater.

#QC_1.1_SCANPY: Note that you can also use the function `pp.calculate_qc_metrics` to compute the fraction of mitochondrial genes by setting `qc_vars = "mito"`. But here we do it another way:


#QC_ALL2:In the same manner we will calculate the proportion gene expression that comes from ribosomal proteins.

#QC_2_SCANPY: Now you can see that we have additional data in the scanpy `obs` slot.

#QC_TITLE2:# Plot QC

#QC_ALL3:Now we can plot some of the QC-features as violin plots.

#QC_ALL4:As you can see, the v2 chemistry gives lower gene detection, but higher detection of ribosomal proteins. As the ribosomal proteins are highly expressed they will make up a larger proportion of the transcriptional landscape when fewer of the lowly expressed genes are detected. And we can plot the different QC-measures as scatter plots.



#-----------





#---------------#
#   FILTERING   #
#---------------#
#FILTERING_TITLE:# Filtering

#FILTERING_TITLE1:## Detection-based filtering

#FILTERING_ALL0:A standard approach is to filter cells with low amount of reads as well as genes that are present in at least a certain amount of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used.

#FILTERING_ALL3: Extremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type. <br>In these datasets, there is also a clear difference between the v2 vs v3 10x chemistry with regards to gene detection, so it may not be fair to apply the same cutoffs to all of them. Also, in the protein assay data there is a lot of cells with few detected genes giving a bimodal distribution. This type of distribution is not seen in the other 2 datasets. Considering that they are all PBMC datasets it makes sense to regard this distribution as low quality libraries. Filter the cells with high gene detection (putative doublets) with cutoffs 4100 for v3 chemistry and 2000 for v2. <br>Here, we will filter the cells with low gene detection (low quality libraries) with less than 1000 genes for v2 and < 500 for v2.

#FILTERING_ALL01:Additionally, we can also see which genes contribute the most to such reads. We can for instance plot the percentage of counts per gene.

#FILTERING_SCATER01:In scater, you can also use the function `plotHighestExprs()` to plot the gene contribution, but the function is quite slow. 

#FILTERING_ALL02:As you can see, MALAT1 constitutes up to 30% of the UMIs from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads, so high detection of MALAT1 may be a technical issue. Let us assemble some information about such genes, which are important for quality control and downstream filtering.


#FILTERING_TITLE2:## Mito/Ribo filtering

#FILTERING_ALL1:We also have quite a lot of cells with high proportion of mitochondrial and low proportion ofribosomal reads. It could be wise to remove those cells, if we have enough cells left after filtering. <br>Another option would be to either remove all mitochondrial reads from the dataset and hope that the remaining genes still have enough biological signal. <br>A third option would be to just regress out the `percent_mito` variable during scaling. In this case we had as much as 99.7% mitochondrial reads in some of the cells, so it is quite unlikely that there is much cell type signature left in those. <br>Looking at the plots, make reasonable decisions on where to draw the cutoff. In this case, the bulk of the cells are below 25% mitochondrial reads and that will be used as a cutoff. We will also remove cells with less than 5% ribosomal reads. 

#FILTERING_ALL2:As you can see, there is still quite a lot of variation in `percent_mito`, so it will have to be dealt with in the data analysis step. We can also notice that the `percent_ribo` are also highly variable, but that is expected since different cell types have different proportions of ribosomal content, according to their function.



#FILTERING_TITLE4:## Plot filtered QC

#FILTERING_ALL5:Lets plot the same QC-stats another time.
#-----------

#FILTERING_TITLE5:## Filter genes

#FILTERING_ALL6:As the level of expression of mitochondrial and MALAT1 genes are judged as mainly technical, it can be wise to remove them from the dataset bofore any further analysis. 

#FILTERING_ALL7:## Save data

#FILTERING_ALL8:Save the filtered dataset to a file for use in later labs.

#----------------#
#   CELL CYCLE   #
#----------------#
#CELLCYCLE_TITLE:# Calculate cell-cycle scores

#CELLCYCLE_ALL1:We here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in data, a score for S phase, a score for G2M phase and the predicted cell cycle phase.

#CELLCYCLE_1_SCANPY:First read the file with cell cycle genes, from Regev lab and split into S and G2M phase genes. Cell cycle genes were retrieved from the scanpy_usage github site via web browser at [RegevLab Github repo](https://github.com/theislab/scanpy_usage/blob/master/180209_cell_cycle/data/regev_lab_cell_cycle_genes.txt).

#CELLCYCLE_2_SCANPY: Before running cell cycle we have to normalize the data. In the scanpy object, the data slot will be overwritten with the normalized data. So first, save the raw data into the slot `raw`.  <br><br>Then run normalization, logarimize and scale the data. 


#CELLCYCLE_3_SCANPY:We here perform cell cycle scoring. The function is actually a wrapper to sc.tl.score_gene_list, which is launched twice, to score separately S and G2M phases. Both sc.tl.score_gene_list and sc.tl.score_cell_cycle_genes are a port from Seurat and are supposed to work in a very similar way. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in data, a score for S phase, a score for G2M phase and the predicted cell cycle phase.

#CELLCYCLE_ALL2:We can now plot a violin plot for the cell cycle scores as well.

#CELLCYCLE_ALL3:In this case it looks like we only have a few cycling cells in the datasets.

#CELLCYCLE_ALL3:# Save data 

#CELLCYCLE_ALL3:Finally, lets save the QC-filtered data for further analysis.

#-----------
```{r setup, include=FALSE}
for( pipeline in c("seurat","scater")){
  lab <- readLines(paste0(script_path,"/",pipeline,"/",pipeline,"_01_qc.Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_01_qc.Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_01_qc.Rmd"))
}
```



#------------------------------#
#   DIMENSIONALITY REDUCTION   #
#------------------------------#

#DIMRED_MAIN:# Dimensionality reduction
#DIMRED_MAIN:
#DIMRED_MAIN:Paulo Czarnewski

#DIMRED_TITLE:## Data preparation
#DIMRED_TITLE:***

#DIMRED_ALL1:First, let's load all necessary libraries and the QC-filtered dataset from the previous step.

#DIMRED_ALL1.1:### Feature selection

#DIMRED_1_SCANPY: Before variable gene selection we need to normalize and logaritmize the data. As in the QC-lab, first store the raw data in the `raw`  slot.

#DIMRED_ALL2:Next, we first need to define which features/genes are important in our dataset to distinguish cell types. For this purpose, we need to find genes that are highly variable across cells, which in turn will also provide a good separation of the cell clusters.

#DIMRED_ALL3:### Z-score transformation

#DIMRED_ALL4:Now that the data is prepared, we now proceed with PCA. Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by PCA. This means that we need to somehow give each gene a similar weight when performing PCA (see below). The common practice is to center and scale each gene before performing PCA. This exact scaling is called Z-score normalization it is very useful for PCA, clustering and plotting heatmaps. <br>Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as `cell cycle`, `sequencing depth`, `percent mitocondria`. This is achieved by doing a generalized linear regression using these parameters as covariates in the model. Then the residuals of the model are taken as the "regressed data". Although perhaps not in the best way, batch effect regression can also be done here.

#DIMRED_SCATER4:By default variables are scaled in the PCA step and is not done separately. But it could be acheieved by running the commads below:

#DIMRED_SCATER1:However, unlike the Seurat, this step is implemented inside the PCA function below. Here we will show you how to add the scaledData back to the object. 

#---------#
#   PCA   #
#---------#
#PCA_TITLE:## PCA
#PCA_TITLE:***
#PCA_ALL1:Performing PCA has many useful applications and interpretations, which much depends on the data used. In the case of life sciences, we want to segregate samples based on gene expression patterns in the data.
#PCA_SEURAT:To run PCA you can use the function `RunPCA()`.
#PCA_SCRAN:As said above, we use the `logcounts` and then set `scale_features` to TRUE in order to scale each gene.
#PCA_SCANPY:To run PCA, you can use the function `pca()`.
#PCA_ALL2:We then plot the first principal components.
#PCA_ALL3:To identify which genes (Seurat) or metadata paramters (Scater/Scran) contribute the most to each PC, one can retreive the loading matrix information. Unfortunatelly this is not implemented in Scater/Scran, so you will need to compute PCA using `logcounts`.
#PCA_3_SCANPY:To identify genes that contribute most to each PC, one can retreive the loading matrix information. 
#PCA_4_SCANPY:The function to plot loading genes only plots genes on the positive axes. Instead plot as a heatmaps, with genes on both postive and negative side, one per pc, and plot their expression amongst cells ordered by their position along the pc.

#PCA_ALL5:We can also plot the amount of variance explained by each PC.
#PCA_ALL6:Based on this plot, we can see that the top 8 PCs retain a lot of information, while other PCs contain pregressivelly less. However, it is still advisable to use more PCs since they might contain informaktion about rare cell types (such as platelets and DCs in this dataset)

#----------#
#   tSNE   #
#----------#
#tSNE_TITLE:## tSNE
#tSNE_TITLE:***
#tSNE_ALL1:We can now run [BH-tSNE](https://arxiv.org/abs/1301.3342).

#tSNE_ALL2:We can now plot the tSNE colored per dataset. We can clearly see the effect of batches present in the dataset.

#----------#
#   UMAP   #
#----------#
#UMAP_TITLE:## UMAP
#UMAP_TITLE:***
#UMAP_ALL1:We can now run [UMAP](https://arxiv.org/abs/1802.03426) for cell embeddings.

#UMAP_1_SCANPY:### Calculate neighborhood graph
#UMAP_1.1_SCANPY:The UMAP implementation in SCANPY uses a neighborhood graph as the distance matrix, so we need to first calculate the graph. 

#UMAP_2_SCANPY:Now we can run UMAP.
#UMAP_ALL2:Another usefullness of UMAP is that it is not limitted by the number of dimensions the data cen be reduced into (unlike tSNE). We can simply reduce the dimentions altering the `n.components` parameter.
#UMAP_ALL2.1:We can now plot the UMAP colored per dataset. Although less distinct as in the tSNE, we still see quite an effect of the different batches in the data.
#UMAP_ALL2.2:We can now plot PCA, UMAP and tSNE side by side for comparison. Here, we can conclude that our dataset contains a batch effect that needs to be corrected before proceeding to clustering and differential gene expression analysis.


#DIMRED_TITLE2:## Using ScaledData and graphs for DR
#DIMRED_TITLE2:***
#DIMRED_ALL5:Althought running a sencond dimmensionality reduction (i.e tSNE or UMAP) on PCA would be a standard approach (because it allows higher computation efficiency), the options are actually limiteless. Below we will show a couple of other common options such as running directly on the scaled data (which was used for PCA) or on a graph built from scaled data. We will show from now on only UMAP, but the same applies for tSNE.

#DIMRED_ALL5.0:### Using ScaledData for UMAP
#DIMRED_ALL5.1:To run tSNE or UMAP on the scaled data, one firts needs to select the number of variables to use. This is because including dimentions that do contribute to the separation of your cell types will in the end mask those differences. Another reason for it is because running with all genes/features also will take longer or might be computationally unfeasible. Therefore we will use the scaled data of the highly variable genes.

#DIMRED_ALL5.2.1:### Using a Graph for UMAP
#DIMRED_ALL5.2:To run tSNE or UMAP on the a graph, we first need to build a graph from the data. In fact, both tSNE and UMAP first build a graph from the data using a specified distance metrix and then optimize the embedding. Since a graph is just a matrix containing distances from cell to cell and as such, you can run either UMAP or tSNE using any other distance metric desired. Euclidean and Correlation are ususally the most commonly used.
#DIMRED_ALL5.3:We can now plot the UMAP comparing both on PCA vs ScaledSata vs Graph.

#DIMRED_TITLE3:## Ploting genes of interest
#DIMRED_TITLE3:***
#DIMRED_ALL6:Let's plot some marker genes for different celltypes onto the embedding. Some genes are:
#MARKER_TABLE:Markers	| Cell Type
#MARKER_TABLE:--- | ---
#MARKER_TABLE:CD3E	| T cells
#MARKER_TABLE:CD3E CD4	| CD4+ T cells
#MARKER_TABLE:CD3E CD8A	| CD8+ T cells
#MARKER_TABLE:GNLY, NKG7	| NK cells
#MARKER_TABLE:MS4A1	| B cells
#MARKER_TABLE:CD14, LYZ, CST3, MS4A7	| CD14+ Monocytes
#MARKER_TABLE:FCGR3A, LYZ, CST3, MS4A7	| FCGR3A+  Monocytes
#MARKER_TABLE:FCER1A, CST3 | DCs

#MARKER_1_SCANPY: The default is to plot gene expression in the normalized and log-transformed data. You can also plot it on the scaled and corrected data by using `use_raw=False`. However, not all of these genes are included in the variable gene set so we first need to filter them. 

#DIMRED_ALL7:We can finally save the object for use in future steps.


```{r setup, include=FALSE}
for( pipeline in c("seurat","scater")){
  lab <- readLines(paste0(script_path,"/",pipeline,"/",pipeline,"_02_dim_reduction.Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_02_dim_reduction_compiled.Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_02_dim_reduction_compiled.Rmd"))
}
```
#-----------











#----------------------#
#   DATA INTEGRATION   #
#----------------------#

#INTEG_TITLE:# Dataset integration
#INTEG_TITLE:
#INTEG_TITLE:Paulo Czarnewski

#INTEG_ALL1:In this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore two different methods to correct for batch effects across datasets. We will also look at a quantitative measure to assess the quality of the integrated data. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of the most recent methods for single data integration:

#INTEG_TABLE:Markdown | Language | Library | Ref
#INTEG_TABLE:--- | --- | --- | ---
#INTEG_TABLE:CCA | R | Seurat | [Cell](https://www.sciencedirect.com/science/article/pii/S0092867419305598?via%3Dihub)
#INTEG_TABLE:MNN | R/Python | Scater/Scanpy | [Nat. Biotech.](https://www.nature.com/articles/nbt.4091)
#INTEG_TABLE:Conos | R | conos | [Nat. Methods](https://www.nature.com/articles/s41592-019-0466-z?error=cookies_not_supported&code=5680289b-6edb-40ad-9934-415dac4fdb2f)
#INTEG_TABLE:Scanorama | Python | scanorama | [Nat. Biotech.](https://www.nature.com/articles/s41587-019-0113-3)

#INTEG_ALL2:Let's first load necessary libraries and the data saved in the previous lab.



#INTEG_ALL3:We split the combined object into a list, with each dataset as an element. We perform standard preprocessing (log-normalization), and identify variable features individually for each dataset based on a variance stabilizing transformation ("vst").

#INTEG_SEURAT1:We identify anchors using the FindIntegrationAnchors function, which takes a list of Seurat objects as input.
#INTEG_SEURAT2:We then pass these anchors to the IntegrateData function, which returns a Seurat object.
#INTEG_SEURAT3:We can observe that a new assay slot is now created under the name `CCA`.
#INTEG_SEURAT4:After running IntegrateData, the Seurat object will contain a new Assay with the integrated (or ‘batch-corrected’) expression matrix. Note that the original (uncorrected values) are still stored in the object in the “RNA” assay, so you can switch back and forth. We can then use this new integrated matrix for downstream analysis and visualization. Here we scale the integrated data, run PCA, and visualize the results with UMAP and TSNE. The integrated datasets cluster by cell type, instead of by technology.


#INTEG_SCRAN1:The mutual nearest neighbors (MNN) approach within the scran package utilizes a novel approach to adjust for batch effects. The `fastMNN()` function returns a representation of the data with reduced dimensionality, which can be used in a similar fashion to other lower-dimensional representations such as PCA. In particular, this representation can be used for downstream methods such as clustering. The BNPARAM can be used to specify the specific nearest neighbors method to use from the BiocNeighbors package. Here we make use of the [Annoy library](https://github.com/spotify/annoy) via the `BiocNeighbors::AnnoyParam()` argument. We save the reduced-dimension MNN representation into the reducedDims slot of our sce object.
#INTEG_SCRAN2:**NOTE**: `fastMNN()` does not produce a batch-corrected expression matrix. 
#INTEG_SCRAN3:We can observe that a new assay slot is now created under the name `MNN`.
#INTEG_SCRAN4:Thus, the result from `fastMNN()` should solely be treated as a reduced dimensionality representation, suitable for direct plotting, TSNE/UMAP, clustering, and trajectory analysis that relies on such results.

#INTEG_1_SCANPY:As the stored AnnData object contains scaled data based on variable genes, we need to make a new object with the raw counts and normalized it again. Variable gene selection should not be performed on the scaled data object, only do normalization and log transformation before variable genes selection. 
#INTEG_2_SCANPY:### Detect variable genes
#INTEG_3_SCANPY:Variable genes can be detected across the full dataset, but then we run the risk of getting many batch-specific genes that will drive a lot of the variation. Or we can select variable genes from each batch separately to get only celltype variation. Here, we will do both as an example of how it can be done. 
#INTEG_4_SCANPY:First we will select genes based on the full dataset.
#INTEG_5_SCANPY:Detect variable genes in each dataset separately using the `batch_key` parameter.
#INTEG_6_SCANPY: Compare overlap of variable genes with batches or with all data.
#INTEG_7_SCANPY:### Data integration
#INTEG_8_SCANPY:First we need to create individual AnnData objects from each of the datasets.
#INTEG_8_SCANPY:Then perform batch correction with MNN.
#INTEG_9_SCANPY:*OBS!* To run mnn, the package mnnpy needs to be installed with `pip install mnnpy`.
#INTEG_10_SCANPY:The mnn_correct function returns a tuple with the AnnData object, list of cell pairs and of angles.Hence, cdata[0] is the new AnnData object. <br> We get corrected expression values for all genes even though only the selected genes were used for finding neighbor cells. For later analysis we want to do dimensionality reduction etc. on the variable genes only, so we will subset the data to only include the variable genes.
#INTEG_11_SCANPY:Now lets run dimensionality reduction on the new integrated object. tSNE and UMAP is run on the new embeddings created by MNN.

#INTEG_12_SCANPY:##Extra exercises
#INTEG_12_SCANPY:###Combat
#INTEG_13_SCANPY:Batch correction can also be performed with combat.<br>Note that ComBat batch correction requires a dense matrix format as input (which is already the case in this example).
#INTEG_14_SCANPY: Variable gene selection, pca and umap with combat data.

#INTEG_15_SCANPY:###Scanorama
#INTEG_16_SCANPY:Try out [Scanorama](https://github.com/brianhie/scanorama) for data integration as well. 
#INTEG_16_SCANPY:To run Scanorama, you need to install python-annoy (already included in conda environment) and scanorama with pip. 
#INTEG_16_SCANPY:We can run scanorama to get a corrected matrix with the `correct` function, or to just get the data projected onto a new common dimension with the function `integrate`. For now, run with just integration.


#INTEG_ALL4:We can now plot the un-integrated and the integrated space reduced dimensions.

#INTEG_ALL5:Finally, lets save the integrated data for further analysis.

```{r setup, include=FALSE}
for( pipeline in c("seurat","scater")){
  lab <- readLines(paste0(script_path,"/",pipeline,"/",pipeline,"_03_integration.Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_03_integration_compiled.Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_03_integration_compiled.Rmd"))
}
```

#-----------











#----------------#
#   CLUSTERING   #
#----------------#
#CLUST_TITLE:# Clustering
#CLUST_TITLE:
#CLUST_ALL:In this tutorial we will continue the analysis of the integrated dataset. We will use the integrated PCA to perform the clustering. First we will construct a $k$-nearest neighbour graph in order to perform a clustering on the graph. We will also show how to perform hierarchical clustering and k-means clustering on PCA space.

#CLUST_1_SCANPY:In this tutorial we will continue the analysis of the integrated dataset. We will use the integrated PCA to perform the clustering using graph community detection algorithms. 


#CLUST_ALL2:Let's first load all necessary libraries and also the integrated dataset from the previous step.



#----------------------#
#   Graph clustering   #
#----------------------#
#CLUST_TITLE2:## Graph clustering
#CLUST_TITLE2:***
#CLUST_ALL3:The procedure of clustering on a Graph can be generalized as 3 main steps:
#CLUST_ALL3:
#CLUST_ALL3:1) Build a kNN graph from the data
#CLUST_ALL3:
#CLUST_ALL3:2) Prune spurious connections from kNN graph (optional step). This is a SNN graph.
#CLUST_ALL3:
#CLUST_ALL3:3) Find groups of cells that maximizes the connections within the group compared other groups.



#CLUST_TITLE2.1:### Building kNN / SNN graph
#CLUST_TITLE2.1:

#CLUST_ALL4:The first step into graph clustering is to construct a k-nn graph, in case you don't have one. For this, we will use the PCA space. Thus, as done for dimensionality reduction, we will use ony the top *N* PCA dimensions for this purpose (the same used for computing UMAP / tSNE).
#CLUST_SEURAT:As we can see above, the **Seurat** function `FindNeighbors` already computes both the KNN and SNN graphs, in which we can control the minimal percentage of shared neighbours to be kept. See `?FindNeighbors` for additional options.
#CLUST_2.1_SCANPY:If you recall from the dimensionality reductionction tutorial, we already constructed a knn graph before running UMAP. Hence we do not need to do it again, and can run the community detection right away. 

#CLUST_ALL4.1:We can take a look at the kNN graph. It is a matrix where every connection between cells is represented as $1$s. This is called a **unweighted** graph (default in Seurat). Some cell connections can however have more importance than others, in that case the scale of the graph from $0$ to a maximum distance. Usually, the smaller the distance, the closer two points are, and stronger is their connection. This is called a **weighted** graph. Both weighted and unweighted graphs are suitable for clustering, but clustering on unweighted graphs is faster for large datasets (> 100k cells).


#CLUST_SCATER2:As you can see, the way Scran computes the SNN graph is different to Seurat. It gives edges to all cells that shares a neighbor, but weights the edges by how similar the neighbors are. Hence, the SNN graph has more edges than the KNN graph.

#CLUST_TITLE2.2:### Clustering on a graph
#CLUST_TITLE2.2:
#CLUST_ALL4.2:Once the graph is built, we can now perform graph clustering. The clustering is done respective to a resolution which can be interpreted as how coarse you want your cluster to be. Higher resolution means higher number of clusters.
#CLUST_SEURAT2:In **Seurat**, the function `FindClusters` will do a graph-based clustering using "Louvain" algorithim by default (`algorithm = 1`). TO use the leiden algorithm, you need to set it to `algorithm = 4`. See `?FindClusters` for additional options.
#CLUST_2_SCANPY:The modularity optimization algoritm in Scanpy are *Leiden* and *Louvain*. Lets test both and see how they compare.
#CLUST_3_SCANPY:###Leiden
#CLUST_4_SCANPY:Plot the clusters, as you can see, with increased resolution, we get higher granularity in the clustering.
#CLUST_5_SCANPY: Once we have done clustering, the relationships between clusters can be calculated as correlation in PCA space and we also visualize some of the marker genes that we used in the Dim Reduction lab onto the clusters.

#CLUST_6_SCANPY:###Louvain



#------------------------#
#   K-means clustering   #
#------------------------#
#CLUST_TITLE3:## K-means clustering
#CLUST_TITLE3:***
#CLUST_ALL7:K-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the nstart argument).




#-----------------------------#
#   Hierarchical clustering   #
#-----------------------------#
#CLUST_TITLE4:## Hierarchical clustering
#CLUST_TITLE4:***
#CLUST_ALL8:Hierarchical clustering is another generic form of clustering that can be applied also to scRNA-seq data. As K-means, it is typically applied to a reduced dimension representation of the data. Hierarchical clustering returns an entire hierarchy of partitionings (a dendrogram) that can be cut at different levels. Hierarchical clustering is done in two steps:
#CLUST_ALL8:
#CLUST_ALL8:* Step1: Define the distances between samples. The most common are Euclidean distance (a.k.a. straight line between two points) or correlation coefficients.
#CLUST_ALL8:* Step2: Define a measure of distances between clusters, called *linkage* criteria. It can for example be average distances between clusters. Commonly used methods are `single`, `complete`, `average`, `median`, `centroid` and `ward`.
#CLUST_ALL8:* Step3: Define the dendrogram among all samples using **Bottom-up** or **Top-down** approach. **Bottom-up** is where samples start with their own cluster which end up merged pair-by-pair until only one cluster is left. **Top-down** is where samples start all in the same cluster that end up being split by 2 until each sample has its own cluster.


#CLUST_TITLE4.1:### Defining distance between cells
#CLUST_ALL8.1:The base R `stats` package already contains a function `dist` that calculates distances between all pairs of samples. Since we want to compute distances between samples, rather than among genes, we need to transpose the data before applying it to the `dist` function. This can be done by simply adding the transpose function `t()` to the data. The distance methods available  in `dist` are: "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski".

#CLUST_ALL8.2:As you might have realized, correlation is not a method implemented in the `dist` function. However, we can create our own distances and transform them to a distance object. We can first compute sample correlations using the `cor` function.

#CLUST_ALL8.2:As you already know, correlation range from -1 to 1, where 1 indicates that two samples are closest, -1 indicates that two samples are the furthest and 0 is somewhat in between. This, however, creates a problem in defining distances because a distance of 0 indicates that two samples are closest, 1 indicates that two samples are the furthest and distance of -1 is not meaningful. We thus need to transform the correlations to a positive scale (a.k.a. **adjacency**):
#CLUST_ALL8.2:
#CLUST_ALL8.2:\[adj = \frac{1- cor}{2}\]
#CLUST_ALL8.2:
#CLUST_ALL8.2:Once we transformed the correlations to a 0-1 scale, we can simply convert it to a distance object using `as.dist` function. The transformation does not need to have a maximum of 1, but it is more intuitive to have it at 1, rather than at any other number.


#CLUST_SCANPY8.2: The function `AgglomerativeClustering` has the option of running with disntance metrics “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed". However, with ward linkage only euklidean distances works. Here we will try out euclidean distance and ward linkage calculated in PCA space.



#CLUST_TITLE4.2:### Clustering cells
#CLUST_ALL8.3:After having calculated the distances between samples calculated, we can now proceed with the hierarchical clustering per-se. We will use the function `hclust` for this purpose, in which we can simply run it with the distance objects created above. The methods available are: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median" or "centroid". It is possible to plot the dendrogram for all cells, but this is very time consuming and we will omit for this tutorial.

#CLUST_ALL8.4: Once your dendrogram is created, the next step is to define which samples belong to a particular cluster. After identifying the dendrogram, we can now literally cut the tree at a fixed threshold (with `cutree`) at different levels to define the clusters. We can either define the number of clusters or decide on a height. We can simply try different clustering levels.

#CLUST_ALL9:Finally, lets save the integrated data for further analysis.

#CLUST_ALL10:## Check QC-stats
#CLUST_ALL10:By now you should know how to plot different features onto your data. Take the QC metrics that were calculated in the first exercise, that should be stored in your data object, and plot it onto your UMAP and as violin plots per cluster using the clustering method of your choice. For example, plot number of UMIS, detected genes, percent mitochondrial reads. 
#CLUST_ALL10:Then, check carefully if there is any bias in how your data is separated due to quality metrics. Could it be explained biologically, or could you have technical bias there?



#--------------------------------------#
#   K-means + Hierachical clustering   #
#--------------------------------------#
#CLUST_TITLE5:### K-means + Hierachical clustering
#CLUST_TITLE5:***




#------------------------------------#
#   Testing Clustering robusteness   #
#------------------------------------#
#CLUST_TITLE6:### Testing Clustering robusteness
#CLUST_TITLE6:***



```{r setup, include=FALSE}
for( pipeline in c("seurat","scater")){
  lab <- readLines(paste0(script_path,"/",pipeline,"/",pipeline,"_04_clustering.Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_04_clustering_compiled.Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_04_clustering_compiled.Rmd"))
}
```

#-----------








#----------------------------------#
#   Differential gene expression   #
#----------------------------------#
#DGE_TITLE:# Differential gene expression
#DGE_ALL:In this tutorial we will cover about Differetial gene expression, which comprises an extensive range of topics and methods. In single cell, differential expresison can have multiple functionalities such as of identifying marker genes for cell populations, as well as differentially regulated genes across conditions (healthy vs control). We will also exercise on how to account the batch information in your test.
#DGE_ALL2:We can first load the data from the clustering session. Moreover, we can already decide which clustering resolution to use. First let's define using the `louvain_2` clustering to identifying differentially expressed genes.  


#DGE_TITLE1:## Cell marker genes
#DGE_TITLE1:***

#DGE_ALL3:Let us first compute a ranking for the highly differential genes in each cluster. There are many different tests and parameters to be chosen that can be used to refine your results. When looking for marker genes, we want genes that are positivelly expressed in a cell type and possibly not expressed in the others.

#DGE_ALL4:We can now select the top 25 up regulated genes for plotting.

#DGE_ALL4.1:We can visualize them as a heatmap. Here we are selecting the top 5.

#DGE_ALL4.2:Another way is by representing the overal group expression and detection rates in a dot-plot.

#DGE_ALL4.3:We can also plot a violin plot for each gene.

#DGE_ALL5:<style>
#DGE_ALL5:div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 10px;}
#DGE_ALL5:</style>
#DGE_ALL5:<div class = "blue">
#DGE_ALL5:**Your turn**
#DGE_ALL5:
#DGE_ALL5:Take a screen shot of those results and re-run the same code above with another test: "wilcox" (Wilcoxon Rank Sum test), "bimod" (Likelihood-ratio test), "roc" (Identifies 'markers' of gene expression using ROC analysis),"t" (Student's t-test),"negbinom" (negative binomial generalized linear model),"poisson" (poisson generalized linear model), "LR" (logistic regression), "MAST" (hurdle model), "DESeq2" (negative binomial distribution).
#DGE_ALL5:</div>

#DGE_TITLE2:## Differential expression across conditions
#DGE_TITLE2:***

#DGE_ALL6:The second way of computing differential expression is to answer which genes are differentially expressed within a cluster. For example, in our case we have libraries comming from 2 different library preparation methods (batches) and we would like to know which genes are influenced the most in a particular cell type. The same concenpt applies if you have instead two or more biological groups (control vs treated, time#0 vs time#1 vs time#2, etc).
#DGE_ALL6:
#DGE_ALL6:For this end, we will first subset our data for the desired cell cluster, then change the cell identities to the variable of comparison (which now in our case is the "Chemistry").

#DGE_ALL6.1:We can now plot the expression across the "Chemistry".

#DGE_ALL6.2:We can clearly see some patterns across them. Those are the genes that impact the most on your batches (see the dimensionality reduction and integration exercises for more details). We can plot those genes using the integrated and non-integrated UMAP for ilustration.

#DGE_ALL6.3:<style>
#DGE_ALL6.3:div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 10px;}
#DGE_ALL6.3:</style>
#DGE_ALL6.3:<div class = "blue">
#DGE_ALL6.3:**Your turn**
#DGE_ALL6.3:
#DGE_ALL6.3:1) Are the differentially expressed genes across `Chemistry` different for each cluster, or are they all the same genes? Change the cluster to use above and check.
#DGE_ALL6.3:2) How would you visualize the results from the previous question?
#DGE_ALL6.3:</div>


#DGE_ALL10:Finally, lets save the integrated data for further analysis.


<<<<<<< HEAD
=======

>>>>>>> e118a0cf4765a56b47fca2432b91c05790857257
```{r setup, include=FALSE}
for( pipeline in c("seurat","scater")){
  lab <- readLines(paste0(script_path,"/",pipeline,"/",pipeline,"_05_dge.Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_05_dge.Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,"_05_dge.Rmd"))
}
```

#-----------
<<<<<<< HEAD


=======
>>>>>>> e118a0cf4765a56b47fca2432b91c05790857257

#-------------------------------------#
#   Trajectory inference: Slingshot   #
#-------------------------------------#
```{r setup, include=FALSE}
#These steps must be run with the specific Slingshot environment, not the normal one!!!

for( pipeline in c("slingshot")){
  lab <- readLines(paste0(script_path,"/trajectory/",pipeline,".Rmd"))
  lab_text <- readLines(paste0(script_path,"/knit_labs.Rmd"))
  
  u <- grep("[#].*[_].*[:]",lab,value = T)[ grep("[#].*[_].*[:]",lab,value = T) %in% sub(":.*",":",grep("[#].*[_].*[:]",lab_text,value = T)) ]
  
  t_lab <- lab
  for( i in u ){
    j <- grep(i,t_lab)
    temp2 <- sub(i,"",lab_text[grepl(i,lab_text)])
    t_lab <- c( t_lab[1:(j-1)] , temp2 , t_lab[(j+1):length(t_lab)] )
  }
  
  PATH <- paste0(script_path,"/compiled/",pipeline)
  if(!dir.exists(PATH)){dir.create(PATH, recursive = T)}
  writeLines(t_lab,paste0(script_path,"/compiled/",pipeline,"/",pipeline,".Rmd"))
  rmarkdown::render(paste0(script_path,"/compiled/",pipeline,"/",pipeline,".Rmd"))
}
```

#-----------



